# ğŸ¤– Introduction to Large Language Models

## ğŸ§  What Is a Language Model?

A **language model** is a *probabilistic model of text*.  

To illustrate this, consider the sentence:

> â€œI wrote to the zoo to send me a pet. They sent me a ___.â€

ğŸ“š This is the opening line of a childrenâ€™s book.  
What a language model does is **compute a probability distribution** over its vocabulary to predict what word should fill in the blank.  

- ğŸ§© The model knows about a set of words (**vocabulary**).  
- ğŸ¯ It assigns a **probability** to each possible word that could fit the blank.  
- ğŸ”¢ When we feed in a sequence of words, it computes a probability for *every single word* in its vocabulary â€” but none outside it.

---

## ğŸ’ª What Makes a Language Model â€œLargeâ€?

**Large Language Models (LLMs)** are not fundamentally different from regular language models.  

The **â€œLargeâ€** in *LLM* refers to the **number of parameters** in the model â€” the adjustable internal values that determine how the model behaves. âš™ï¸  
However, thereâ€™s **no universal threshold** defining when a language model becomes â€œlargeâ€ or â€œextra-large.â€

In general, when people talk about LLMs, they mean a **particular style** of model used for **text generation** ğŸ—£ï¸.  
That said, some smaller models (like **BERT**) are sometimes still called LLMs â€” even if theyâ€™re not particularly â€œlarge.â€

---

## ğŸ’¬ What Can LLMs Do?

We know that if we provide an LLM with a **sequence of text** (a prefix), it computes a **distribution over possible next words**.  

But there are some key questions ğŸ¤”:

1. ğŸ›ï¸ Can we affect that distribution?  
2. ğŸ§© What mechanisms do we have for changing it?  
3. ğŸ§¾ How does this affect which words the model generates?

Once we have a probability distribution over text, we can use it to **generate new text** â€” sentences, paragraphs, or even full documents! âœï¸

---

## ğŸ§© Key Topics in This Module

This learning module is structured around three main **technical areas**:

### 1ï¸âƒ£ Architecture ğŸ—ï¸
- How are these models built?  
- What do they look like under the hood?  
- What do their architectures imply about what they can do?

### 2ï¸âƒ£ Affecting the Distribution ğŸšï¸
Weâ€™ll explore two primary methods for influencing an LLMâ€™s output:

- ğŸª„ **Prompting** â€” does *not* change the modelâ€™s parameters.  
- ğŸ§  **Training** â€” *does* change the modelâ€™s parameters.

### 3ï¸âƒ£ Decoding ğŸ”¡
- **Decoding** is the process of generating text from an LLM.  
- It involves sampling from the modelâ€™s vocabulary in creative and structured ways to produce coherent text. ğŸ’¬
---

## ğŸ” Extensions and Research Directions

Finally, weâ€™ll look at **extensions** of these core ideas and explore how researchers in academia and industry are advancing LLMs in **new and innovative ways**. ğŸ§ªâœ¨

---

## âœ… Summary

In this module, we have:
- Defined what large language models are. ğŸ“˜  
- Outlined the core topics: **architecture**, **prompting/training**, and **decoding**.  
- Previewed upcoming lessons that will dive deeper into these areas.


