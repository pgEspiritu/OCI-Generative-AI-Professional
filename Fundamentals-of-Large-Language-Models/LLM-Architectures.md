# ğŸ§± Lesson 2: Understanding LLM Architectures

In this lesson, weâ€™ll talk about **LLM architectures** â€” focusing on two major types:

1. ğŸ§© **Encoders**
2. ğŸ”¡ **Decoders**

These two architectures correspond to two important capabilities of language models:

- âœ´ï¸ **Embedding**
- ğŸ—£ï¸ **Text Generation**

Before diving deeper, remember that **both encoders and decoders** are built on a foundational structure called the **Transformer**.

---

## âš™ï¸ What Is a Transformer?

The **Transformer architecture**, introduced in the landmark 2017 paper *â€œAttention Is All You Needâ€*, completely revolutionized Natural Language Processing (NLP) and machine learning.  

While we wonâ€™t go into the full details of the Transformer here, there are plenty of great tutorials available if youâ€™d like to explore this architecture further. ğŸ’¡

---

## ğŸ§  Encoders vs. Decoders â€” The Basics

| Architecture | Purpose | Example Models | Key Use Case |
|---------------|----------|----------------|--------------|
| **Encoder** | Converts text into numerical **embeddings** | BERT, RoBERTa | Classification, Search, Semantic Similarity |
| **Decoder** | **Generates text** token by token | GPT-4, Llama, Cohere Command | Text Generation, Chatbots, Summarization |
| **Encoder-Decoder** | Combines both for sequence-to-sequence tasks | T5, BART | Translation, Summarization, Complex NLP Pipelines |

---

## ğŸª„ What Are Text Embeddings?

When we talk about *embedding text*, weâ€™re referring to converting a sequence of words into a **vector (or a sequence of vectors)** â€” a **numerical representation** that captures the *meaning* (semantics) of the text.  

For example, given the input:

> â€œThey sent me a ...â€

An encoder model like **BERT** produces:
- A **vector for each word**, and  
- A **vector for the whole sentence** ğŸ§­  

These embeddings can be used for:
- ğŸ” **Semantic search** â€” finding similar documents in a corpus  
- ğŸ·ï¸ **Classification** or **regression** tasks  
- ğŸ§© **Clustering** or **recommendation systems**

---

## ğŸ§© Encoders in Action

Letâ€™s walk through an example of how an **encoder** like **BERT** works:

1. ğŸ“¥ Input text: `"They sent me a"`
2. ğŸ§  The model converts each token into an **embedding vector**
3. ğŸ§® These vectors represent semantic meaning
4. ğŸ“¤ These embeddings are then used by downstream models or systems (e.g., search, categorization, etc.)

For **semantic search**, you might:
- Encode all documents in a database  
- Encode the userâ€™s query  
- Compare vectors to find the *most similar* document ğŸ§·

---

## ğŸ’¬ Decoders: The Text Generators

Now letâ€™s talk about **decoders**, such as **GPT-4**, **Llama**, or **Cohere Command**.

Decoders take a **sequence of tokens** and **predict the next token** based on their internal probability distribution â€” exactly as discussed in Lesson 1.

ğŸ§© **Key characteristic:**  
A decoder **only generates one token at a time**!  

To generate a full sequence:
1. Feed in an initial text (prompt).  
2. The model outputs the **next token**.  
3. Append that token to the input and feed it back in.  
4. Repeat until the full sequence is generated. ğŸ”  

ğŸ’¡ While powerful, this process can be **computationally expensive**, especially for very large models.

> ğŸ“ Typically, **decoders** are *not* used for embedding tasks â€” thatâ€™s the job of **encoders**.

---

## ğŸš€ Why Are Decoders So Popular?

Decoders have shown **remarkable fluency** in:
- Generating coherent, human-like text âœï¸  
- Answering questions â“  
- Engaging in dialogue ğŸ’¬  
- Performing reasoning tasks ğŸ§®  

Theyâ€™re the backbone of todayâ€™s conversational AI systems â€” including models like ChatGPT.

---

## ğŸ”„ The Hybrid: Encoderâ€“Decoder Models

**Encoderâ€“Decoder models** combine the strengths of both architectures.  
Theyâ€™re especially effective for **sequence-to-sequence** tasks like **translation**. ğŸŒ

Hereâ€™s how it works:
1. ğŸ§  The **encoder** processes input text (e.g., English sentence) and converts it into embeddings.  
2. ğŸ”¡ The **decoder** takes those embeddings and generates the translated text (e.g., in French).  
3. ğŸ” The decoder generates one token at a time, feeding its own outputs back into the sequence until completion.

---

## ğŸ“Š Model Size and Scale

In LLMs, **size** refers to the number of **trainable parameters**.  

A few key observations:
- âš–ï¸ **Decoder models** are usually much **larger** than encoders.  
- ğŸ“ˆ The **y-axis** in most parameter-count charts grows **by orders of magnitude**, not linearly.  
- ğŸ§® Bigger isnâ€™t always better â€” small models can perform surprisingly well with clever training techniques.

Recent research shows that with smarter methods, **smaller models** can still generate fluent, high-quality text! ğŸ’¡âœ¨

---

## ğŸ§¾ Choosing the Right Architecture

At a glance, hereâ€™s a quick guide on which model type is typically used for each task:

| Task | Encoder | Decoder | Encoder-Decoder |
|------|----------|----------|----------------|
| Text Classification | âœ… | âŒ | âœ… |
| Text Generation | âŒ | âœ… | âœ… |
| Translation | âŒ | âŒ | âœ… |
| Semantic Search | âœ… | âŒ | âŒ |
| Summarization | âŒ | âœ… | âœ… |
| Dialogue / Chatbots | âŒ | âœ… | âœ… |

> âš ï¸ Note: Any model *could* theoretically perform any task, but not all combinations are efficient or commonly used.

---

## âœ… Lesson Summary

- ğŸ§© **Encoders** convert text into embeddings.  
- ğŸ”¡ **Decoders** generate text, token by token.  
- ğŸ”€ **Encoderâ€“Decoder** models combine both for translation and sequence tasks.  
- âš™ï¸ All are built upon the **Transformer** architecture.  
- ğŸ§® Model **size** (parameters) varies widely, with decoders often being the largest.  
Â© 2025 Oracle University  
[ğŸ”’ Privacy / Do Not Sell My Info](#) Â· [ğŸ“ Contact Us](#) Â· [ğŸ“œ Terms of Use](#) Â· [ğŸª Cookie Preferences](#) Â· [âš™ï¸ Ad Choices](#)
